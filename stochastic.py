import csv
import numpy as np


def open_dataset(path: str):
    """ 
        ouvre le dataset du path
        recup les infos dans un bloc divise par matiere
        remplace les cases vides par None
        ne prend pas en compte les erreurs
    """
    
    csvfile = open(path, newline='')
    file = csv.reader(csvfile, delimiter=',', quotechar='|')
    
    for i, line in enumerate(file):
        if i == 0:
            dataset = [[] for _ in range(len(line))]
            pass
        for y, elem in enumerate(line):
            if len(elem) == 0:
                dataset[y].append(None)
                continue
            dataset[y].append(elem)
    return dataset


def sort_by_kind(dataset, index):
    """
        prend un dataset et les tri par rapport au donner du champs
        ex: je veux trier par maisons tu donne l'index du champ maison
            le prog cherche le nombre de diff maisons et les trie
            dataset => maison[dataset,Ravenclaw,Slytherin,Gryffindor,Hufflepuff]
        le retour est un tableau de taille variable dans un ordre radom a cause du set()
        avec comme 0 le dataset de base
    """
    names = list(set(dataset[index][1:]))
    sorted = [[] for _ in range(len(names))]
    for stud in range(len(dataset[index])):
        # print(i)
        if dataset[index][stud] in names:
            sorted[names.index(dataset[index][stud])].append(stud)
            # print(stud)
            
        
    return sorted, names

def get_data(dataset, index, to):
    """
        retourne une liste de valeurs du dataset a l'index de chaque membre de to
    """
    values = []
    for elem in to:
        values.append(dataset[index][elem])
    return values


def sigmoid(z):
    """
    Fonction sigmo√Øde pour la r√©gression logistique
    """
    # Clip pour √©viter les overflow
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))


def prepare_data(dataset, feature_indices, target_index):
    """
    Pr√©pare les donn√©es pour l'entra√Ænement
    - dataset: le dataset complet
    - feature_indices: indices des colonnes √† utiliser comme features
    - target_index: index de la colonne target (maisons)
    """
    X = []
    y = []
    
    # R√©cup√©rer les noms des maisons uniques
    houses = list(set(dataset[target_index][1:]))  # Skip header
    house_to_int = {house: i for i, house in enumerate(houses)}
    
    for i in range(1, len(dataset[0])):  # Skip header row
        # V√©rifier que toutes les features sont disponibles
        row_features = []
        skip_row = False
        
        for feat_idx in feature_indices:
            if dataset[feat_idx][i] is None or dataset[feat_idx][i] == '':
                skip_row = True
                break
            try:
                row_features.append(float(dataset[feat_idx][i]))
            except ValueError:
                skip_row = True
                break
        
        # V√©rifier que le target est disponible
        if dataset[target_index][i] is None or dataset[target_index][i] == '':
            skip_row = True
        
        if not skip_row:
            X.append(row_features)
            y.append(house_to_int[dataset[target_index][i]])
    
    return np.array(X), np.array(y), houses, house_to_int


def normalize_features(X):
    """
    Normalise les features (standardisation)
    """
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    # √âviter la division par z√©ro
    std = np.where(std == 0, 1, std)
    X_normalized = (X - mean) / std
    return X_normalized, mean, std


def one_vs_all_logistic_regression(X, y, num_classes, learning_rate=0.01, epochs=1000):
    """
    R√©gression logistique One-vs-All pour classification multi-classe
    """
    n_samples, n_features = X.shape
    
    # Initialiser les poids pour chaque classe
    weights = np.random.normal(0, 0.01, (num_classes, n_features))
    biases = np.zeros(num_classes)
    
    # Historique des co√ªts
    cost_history = []
    
    for epoch in range(epochs):
        # M√©langer les donn√©es pour SGD
        indices = np.random.permutation(n_samples)
        epoch_cost = 0
        
        for i in indices:
            x_i = X[i]
            y_i = y[i]
            
            # Pour chaque classe (One-vs-All)
            for class_idx in range(num_classes):
                # Target binaire: 1 si c'est la classe courante, 0 sinon
                y_binary = 1 if y_i == class_idx else 0
                
                # Forward pass
                z = np.dot(x_i, weights[class_idx]) + biases[class_idx]
                prediction = sigmoid(z)
                
                # Calculer le co√ªt (log-loss)
                cost = -y_binary * np.log(prediction + 1e-15) - (1 - y_binary) * np.log(1 - prediction + 1e-15)
                epoch_cost += cost
                
                # Backward pass (gradients)
                error = prediction - y_binary
                dw = error * x_i
                db = error
                
                # Mise √† jour des poids (SGD)
                weights[class_idx] -= learning_rate * dw
                biases[class_idx] -= learning_rate * db
        
        # Enregistrer le co√ªt moyen de l'√©poque
        avg_cost = epoch_cost / (n_samples * num_classes)
        cost_history.append(avg_cost)
        
        # Afficher le progr√®s
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Cost: {avg_cost:.4f}")
    
    return weights, biases, cost_history


def predict(X, weights, biases):
    """
    Faire des pr√©dictions avec le mod√®le entra√Æn√©
    """
    n_samples = X.shape[0]
    num_classes = weights.shape[0]
    
    # Calculer les scores pour chaque classe
    scores = np.zeros((n_samples, num_classes))
    for class_idx in range(num_classes):
        z = np.dot(X, weights[class_idx]) + biases[class_idx]
        scores[:, class_idx] = sigmoid(z)
    
    # Prendre la classe avec le score le plus √©lev√©
    predictions = np.argmax(scores, axis=1)
    return predictions, scores


def calculate_accuracy(y_true, y_pred):
    """
    Calculer la pr√©cision
    """
    return np.mean(y_true == y_pred)


def train_hogwarts_classifier(dataset):
    """
    Entra√Æner un classificateur pour les maisons de Poudlard
    """
    print("=== ENTRA√éNEMENT DU CLASSIFICATEUR POUDLARD ===")
    
    # Utiliser plus de features pour am√©liorer la pr√©cision
    # Toutes les mati√®res importantes bas√©es sur l'analyse
    feature_indices = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]  # Toutes les mati√®res
    feature_names = ["Arithmancy", "Astronomy", "Herbology", "Defense Against the Dark Arts", 
                    "Divination", "Muggle Studies", "Ancient Runes", "History of Magic",
                    "Transfiguration", "Potions", "Care of Magical Creatures", "Charms", "Flying"]
    target_index = 1  # Index de la colonne des maisons
    
    # Pr√©parer les donn√©es
    print("Pr√©paration des donn√©es...")
    X, y, houses, house_to_int = prepare_data(dataset, feature_indices, target_index)
    print(f"Donn√©es pr√©par√©es: {X.shape[0]} √©chantillons, {X.shape[1]} features")
    print(f"Features utilis√©es: {feature_names}")
    print(f"Maisons: {houses}")
    
    # Normaliser les features
    X_norm, mean, std = normalize_features(X)
    
    # Diviser en train/validation (85/15 pour plus de donn√©es d'entra√Ænement)
    split_idx = int(0.85 * len(X))
    indices = np.random.permutation(len(X))
    
    X_train = X_norm[indices[:split_idx]]
    y_train = y[indices[:split_idx]]
    X_val = X_norm[indices[split_idx:]]
    y_val = y[indices[split_idx:]]
    
    print(f"Train set: {len(X_train)} √©chantillons")
    print(f"Validation set: {len(X_val)} √©chantillons")
    
    # Entra√Æner le mod√®le avec hyperparam√®tres optimis√©s
    print("\nEntra√Ænement en cours...")
    weights, biases, cost_history = one_vs_all_logistic_regression(
        X_train, y_train, 
        num_classes=len(houses),
        learning_rate=0.3,  # Learning rate plus √©lev√©
        epochs=2000         # Plus d'√©poques
    )
    
    # √âvaluer sur le set de validation
    print("\n√âvaluation...")
    y_pred_train, _ = predict(X_train, weights, biases)
    y_pred_val, scores_val = predict(X_val, weights, biases)
    
    train_acc = calculate_accuracy(y_train, y_pred_train)
    val_acc = calculate_accuracy(y_val, y_pred_val)
    
    print(f"Pr√©cision sur l'entra√Ænement: {train_acc:.4f} ({train_acc*100:.2f}%)")
    print(f"Pr√©cision sur la validation: {val_acc:.4f} ({val_acc*100:.2f}%)")
    
    # V√©rifier si on atteint l'objectif de 98%
    if val_acc < 0.98:
        print(f"‚ö†Ô∏è  Objectif de 98% non atteint (actuel: {val_acc*100:.2f}%)")
        print("Tentative avec des param√®tres plus agressifs...")
        
        # R√©entra√Æner avec des param√®tres plus agressifs
        weights, biases, cost_history = one_vs_all_logistic_regression(
            X_train, y_train, 
            num_classes=len(houses),
            learning_rate=0.5,
            epochs=3000
        )
        
        y_pred_train, _ = predict(X_train, weights, biases)
        y_pred_val, scores_val = predict(X_val, weights, biases)
        
        train_acc = calculate_accuracy(y_train, y_pred_train)
        val_acc = calculate_accuracy(y_val, y_pred_val)
        
        print(f"Nouvelle pr√©cision sur l'entra√Ænement: {train_acc:.4f} ({train_acc*100:.2f}%)")
        print(f"Nouvelle pr√©cision sur la validation: {val_acc:.4f} ({val_acc*100:.2f}%)")
    
    if val_acc >= 0.98:
        print(f"‚úÖ Objectif de 98% atteint ! ({val_acc*100:.2f}%)")
    
    # Afficher quelques pr√©dictions
    print("\n=== EXEMPLES DE PR√âDICTIONS ===")
    for i in range(min(10, len(X_val))):
        true_house = houses[y_val[i]]
        pred_house = houses[y_pred_val[i]]
        confidence = scores_val[i][y_pred_val[i]]
        status = "‚úÖ" if true_house == pred_house else "‚ùå"
        print(f"{status} Vrai: {true_house:12} | Pr√©dit: {pred_house:12} | Confiance: {confidence:.3f}")
    
    return weights, biases, mean, std, houses, house_to_int, feature_names


def save_model(weights, biases, mean, std, houses, house_to_int, feature_names, filename="model.npz"):
    """
    Sauvegarder le mod√®le entra√Æn√©
    """
    np.savez(filename, 
             weights=weights, 
             biases=biases, 
             mean=mean, 
             std=std, 
             houses=houses, 
             house_to_int=list(house_to_int.items()),
             feature_names=feature_names)
    print(f"Mod√®le sauvegard√© dans {filename}")
    
    # Sauvegarder aussi en format lisible
    weights_readable_file = filename.replace('.npz', '_weights.txt')
    save_weights_readable(weights, biases, houses, feature_names, weights_readable_file)


def save_weights_readable(weights, biases, houses, feature_names, filename="weights.txt"):
    """
    Sauvegarder les poids dans un format lisible
    """
    with open(filename, 'w') as f:
        f.write("="*60 + "\n")
        f.write("      POIDS DU MOD√àLE R√âGRESSION LOGISTIQUE POUDLARD\n")
        f.write("="*60 + "\n\n")
        
        f.write("Features utilis√©es:\n")
        for i, feature in enumerate(feature_names):
            f.write(f"  {i:2d}: {feature}\n")
        f.write("\n")
        
        f.write("POIDS PAR MAISON:\n")
        f.write("-" * 60 + "\n")
        
        for house_idx, house in enumerate(houses):
            f.write(f"\nüè† {house.upper()}:\n")
            f.write(f"   Biais: {biases[house_idx]:8.4f}\n")
            f.write("   Poids des mati√®res:\n")
            
            # Trier les poids par ordre d'importance (valeur absolue)
            weight_importance = [(abs(weights[house_idx][i]), i, weights[house_idx][i]) 
                               for i in range(len(feature_names))]
            weight_importance.sort(reverse=True)
            
            for abs_weight, feat_idx, weight in weight_importance:
                influence = "üìà Tr√®s forte" if abs_weight > 2 else "üìä Forte" if abs_weight > 1 else "üìâ Moyenne" if abs_weight > 0.5 else "üìã Faible"
                sign = "‚ûï" if weight > 0 else "‚ûñ"
                f.write(f"     {sign} {feature_names[feat_idx]:25}: {weight:8.4f} ({influence})\n")
        
        f.write("\n" + "="*60 + "\n")
        f.write("INTERPR√âTATION:\n")
        f.write("‚ûï Poids positif = Plus cette mati√®re a une note √©lev√©e, plus l'√©tudiant\n")
        f.write("   a de chances d'√™tre dans cette maison\n")
        f.write("‚ûñ Poids n√©gatif = Plus cette mati√®re a une note √©lev√©e, moins l'√©tudiant\n")
        f.write("   a de chances d'√™tre dans cette maison\n")
        f.write("="*60 + "\n")
    
    print(f"Poids sauvegard√©s en format lisible: {filename}")


def display_weights(weights, biases, houses, feature_names):
    """
    Afficher les poids de mani√®re lisible dans la console
    """
    print("\n" + "="*60)
    print("           ANALYSE DES POIDS DU MOD√àLE")
    print("="*60)
    
    for house_idx, house in enumerate(houses):
        print(f"\nüè† {house.upper()}:")
        print(f"   Biais: {biases[house_idx]:8.4f}")
        print("   Top 5 mati√®res les plus influentes:")
        
        # Trier par importance
        weight_importance = [(abs(weights[house_idx][i]), i, weights[house_idx][i]) 
                           for i in range(len(feature_names))]
        weight_importance.sort(reverse=True)
        
        for i, (abs_weight, feat_idx, weight) in enumerate(weight_importance[:5]):
            sign = "‚ûï" if weight > 0 else "‚ûñ"
            print(f"     {i+1}. {sign} {feature_names[feat_idx]:25}: {weight:8.4f}")


def load_model(filename="model.npz"):
    """
    Charger un mod√®le pr√©-entra√Æn√©
    """
    data = np.load(filename, allow_pickle=True)
    weights = data['weights']
    biases = data['biases']
    mean = data['mean']
    std = data['std']
    houses = data['houses'].tolist()
    house_to_int = dict(data['house_to_int'].tolist())
    feature_names = data['feature_names'].tolist() if 'feature_names' in data else ["Feature_" + str(i) for i in range(weights.shape[1])]
    print(f"Mod√®le charg√© depuis {filename}")
    return weights, biases, mean, std, houses, house_to_int, feature_names


def predict_test_file(test_file_path, model_filename="model.npz", output_file="houses.csv"):
    """
    Faire des pr√©dictions sur un fichier de test et sauvegarder les r√©sultats
    """
    print(f"=== PR√âDICTION SUR {test_file_path} ===")
    
    # Charger le mod√®le
    weights, biases, mean, std, houses, house_to_int, feature_names = load_model(model_filename)
    
    # Charger les donn√©es de test
    test_data = open_dataset(test_file_path)
    
    # Utiliser les m√™mes features que pour l'entra√Ænement
    feature_indices = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]  # Toutes les mati√®res
    
    # Pr√©parer les donn√©es de test
    X_test = []
    valid_indices = []  # Pour garder trace des lignes valides
    
    print("Pr√©paration des donn√©es de test...")
    for i in range(1, len(test_data[0])):  # Skip header
        row_features = []
        skip_row = False
        
        for feat_idx in feature_indices:
            if test_data[feat_idx][i] is None or test_data[feat_idx][i] == '':
                skip_row = True
                break
            try:
                row_features.append(float(test_data[feat_idx][i]))
            except ValueError:
                skip_row = True
                break
        
        if not skip_row:
            X_test.append(row_features)
            valid_indices.append(i)
    
    X_test = np.array(X_test)
    print(f"Donn√©es de test pr√©par√©es: {len(X_test)} √©chantillons valides sur {len(test_data[0])-1}")
    
    # Normaliser avec les param√®tres du mod√®le d'entra√Ænement
    X_test_norm = (X_test - mean) / std
    
    # Faire les pr√©dictions
    predictions, scores = predict(X_test_norm, weights, biases)
    predicted_houses = [houses[pred] for pred in predictions]
    
    # Cr√©er le fichier de r√©sultats
    print(f"Sauvegarde des pr√©dictions dans {output_file}...")
    with open(output_file, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Index', 'Hogwarts House'])  # Header
        
        prediction_idx = 0
        for i in range(1, len(test_data[0])):  # Pour chaque ligne du fichier original
            if i in valid_indices:
                # On a une pr√©diction pour cette ligne
                index = test_data[0][i]  # Index original
                house = predicted_houses[prediction_idx]
                writer.writerow([index, house])
                prediction_idx += 1
            else:
                # Ligne avec donn√©es manquantes - pr√©diction par d√©faut
                index = test_data[0][i]
                writer.writerow([index, "Hufflepuff"])  # Maison par d√©faut
    
    print(f"Pr√©dictions termin√©es ! Fichier sauv√©: {output_file}")
    
    # Afficher quelques statistiques
    print("\n=== STATISTIQUES DES PR√âDICTIONS ===")
    house_counts = {}
    for house in predicted_houses:
        house_counts[house] = house_counts.get(house, 0) + 1
    
    for house, count in house_counts.items():
        percentage = (count / len(predicted_houses)) * 100
        print(f"{house:12}: {count:4} √©tudiants ({percentage:.1f}%)")
    
    # Afficher quelques exemples avec confiance
    print("\n=== EXEMPLES DE PR√âDICTIONS ===")
    for i in range(min(10, len(predicted_houses))):
        idx = valid_indices[i]
        student_index = test_data[0][idx]
        first_name = test_data[2][idx] if test_data[2][idx] else "?"
        last_name = test_data[3][idx] if test_data[3][idx] else "?"
        predicted_house = predicted_houses[i]
        confidence = scores[i][predictions[i]]
        
        print(f"Index {student_index}: {first_name} {last_name} ‚Üí {predicted_house} (confiance: {confidence:.3f})")
    
    return predicted_houses


def train_and_save_model(dataset_path="datasets/dataset_train.csv", model_filename="model.npz"):
    """
    Entra√Æner un mod√®le et le sauvegarder
    """
    # Charger et entra√Æner
    data = open_dataset(dataset_path)
    weights, biases, mean, std, houses, house_to_int, feature_names = train_hogwarts_classifier(data)
    
    # Afficher l'analyse des poids
    display_weights(weights, biases, houses, feature_names)
    
    # Sauvegarder
    save_model(weights, biases, mean, std, houses, house_to_int, feature_names, model_filename)
    
    return weights, biases, mean, std, houses, house_to_int, feature_names


# Test de l'algorithme de r√©gression logistique
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # Mode pr√©diction sur fichier de test
        if sys.argv[1] == "predict":
            if len(sys.argv) < 3:
                print("Usage: python stochastic.py predict <test_file.csv> [output_file.csv]")
                sys.exit(1)
            
            test_file = sys.argv[2]
            output_file = sys.argv[3] if len(sys.argv) > 3 else ".csv"
            
            # V√©rifier si le mod√®le existe
            try:
                predict_test_file(test_file, "model.npz", output_file)
            except FileNotFoundError:
                print("Mod√®le non trouv√© ! Entra√Ænement d'un nouveau mod√®le...")
                train_and_save_model()
                predict_test_file(test_file, "model.npz", output_file)
        
        elif sys.argv[1] == "train":
            # Mode entra√Ænement seulement
            train_file = sys.argv[2] if len(sys.argv) > 2 else "datasets/dataset_train.csv"
            train_and_save_model(train_file)
        
        elif sys.argv[1] == "weights":
            # Mode affichage des poids
            model_file = sys.argv[2] if len(sys.argv) > 2 else "model.npz"
            try:
                weights, biases, mean, std, houses, house_to_int, feature_names = load_model(model_file)
                display_weights(weights, biases, houses, feature_names)
                
                # Proposer de sauvegarder en format lisible
                save_readable = input("\nVoulez-vous sauvegarder les poids en format lisible ? (o/n): ").lower()
                if save_readable in ['o', 'oui', 'y', 'yes']:
                    readable_file = model_file.replace('.npz', '_weights.txt')
                    save_weights_readable(weights, biases, houses, feature_names, readable_file)
            except FileNotFoundError:
                print(f"Mod√®le {model_file} non trouv√© !")
        
        else:
            print("Commandes disponibles:")
            print("  python stochastic.py train [dataset.csv]     - Entra√Æner un mod√®le")
            print("  python stochastic.py predict <test.csv>      - Faire des pr√©dictions") 
            print("  python stochastic.py weights [model.npz]     - Afficher les poids")
            print("  python stochastic.py                         - Mode d√©monstration")
    
    else:
        # Mode par d√©faut: entra√Ænement + d√©monstration
        # Charger les donn√©es
        data = open_dataset("datasets/dataset_train.csv")
        
        # Afficher les colonnes pour debug
        print("Colonnes du dataset:")
        for i, col in enumerate(data):
            print(f"{i}: {col[0]}")
        
        # Entra√Æner le classificateur
        weights, biases, mean, std, houses, house_to_int, feature_names = train_hogwarts_classifier(data)
        
        print("\n=== MOD√àLE ENTRA√éN√â ===")
        
        # Afficher l'analyse d√©taill√©e des poids
        display_weights(weights, biases, houses, feature_names)
        
        # Sauvegarder le mod√®le
        save_model(weights, biases, mean, std, houses, house_to_int, feature_names)
        
        # Test sur le fichier de test s'il existe
        try:
            print("\n" + "="*50)
            predict_test_file("datasets/dataset_test.csv", "model.npz", "houses.csv")
        except FileNotFoundError:
            print("\nFichier de test non trouv√©. Pour faire des pr√©dictions:")
            print("python stochastic.py predict <test_file.csv> [output_file.csv]")

